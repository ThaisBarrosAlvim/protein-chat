{
 "cells": [
  {
   "cell_type": "code",
   "id": "8a999514",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "!pip install -q -U bitsandbytes transformers peft accelerate datasets scipy einops evaluate trl rouge_score ipywidgets"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa7275f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "id": "63ed2e8a",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "from pyarrow.dataset import dataset\n",
    "from sympy.codegen.ast import continue_\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    GenerationConfig, TFOpenAIGPTDoubleHeadsModel\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from trl import SFTTrainer\n",
    "import json\n",
    "import torch\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from huggingface_hub import interpreter_login\n",
    "from datasets import load_dataset\n",
    "\n",
    "interpreter_login()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e343e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# disable Weights and Biases # TODO evaluate using WB\n",
    "os.environ['WANDB_DISABLED']=\"true\"\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF']=\"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "id": "371aa4f1",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "from time import sleep\n",
    "\n",
    "progress_bar = IntProgress(min=0, max=100)\n",
    "display(progress_bar)\n",
    "\n",
    "for i in range(100):\n",
    "    progress_bar.value = i\n",
    "    sleep(0.1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b85f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega o dataset e divide ele em treino, validação e teste (TODO montar o dataset já desta forma)\n",
    "dataset = load_dataset('json', data_files='../data_construction/dataset_protein_articles_trunc.jsonl', split='train')\n",
    "\n",
    "# Definir proporções simplificadas (4:1:1) \n",
    "train_ratio = 4 / (4 + 1 + 1)\n",
    "test_ratio = 1 / (4 + 1 + 1)\n",
    "validation_ratio = 1 / (4 + 1 + 1)\n",
    "\n",
    "# Primeiro, dividir o dataset entre treino e (teste + validação)\n",
    "train_val_test = dataset.train_test_split(test_size=(1 - train_ratio))\n",
    "\n",
    "# Dividir a parte restante entre teste e validação\n",
    "test_val = train_val_test['test'].train_test_split(test_size=validation_ratio / (test_ratio + validation_ratio))\n",
    "\n",
    "# Obter os datasets de treino, teste e validação\n",
    "train_dataset = train_val_test['train']\n",
    "test_dataset = test_val['train']\n",
    "validation_dataset = test_val['test']\n",
    "\n",
    "# Exibir o tamanho de cada split\n",
    "print(f\"Tamanho do treino: {len(train_dataset)}\")\n",
    "print(f\"Tamanho do teste: {len(test_dataset)}\")\n",
    "print(f\"Tamanho da validação: {len(validation_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf899d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, \"float16\")\n",
    "model_name='meta-llama/Llama-3.2-3B'\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df42347d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device_map = {\"\": 0}\n",
    "original_model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                                      device_map=device_map,\n",
    "                                                      quantization_config=bnb_config,\n",
    "                                                      trust_remote_code=True,\n",
    "                                                      token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a7dc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name,trust_remote_code=True,padding_side=\"left\",add_eos_token=True,add_bos_token=True,use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4db848c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def get_max_length(model):\n",
    "    conf = model.config\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max lenth: {max_length}\")\n",
    "            break\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length\n",
    "\n",
    "\n",
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizing a batch\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int,seed, dataset):\n",
    "    \"\"\"Format & tokenize it so it is ready for training\n",
    "    :param tokenizer (AutoTokenizer): Model Tokenizer\n",
    "    :param max_length (int): Maximum number of tokens to emit from tokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    # Add prompt to each sample\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    dataset = dataset.map(create_prompt_formats)#, batched=True)\n",
    "\n",
    "    # Apply preprocessing to each batch of the dataset & and remove 'instruction', 'context', 'response', 'category' fields\n",
    "    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        remove_columns=['instruction', 'input', 'output'],\n",
    "    )\n",
    "\n",
    "    # Filter out samples that have input_ids exceeding max_length\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "\n",
    "    # Shuffle dataset\n",
    "    dataset = dataset.shuffle(seed=seed)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def create_prompt_formats(sample):\n",
    "    \"\"\"\n",
    "    Format various fields of the sample ('instruction','output')\n",
    "    Then concatenate them using two newline characters \n",
    "    :param sample: Sample dictionnary\n",
    "    \"\"\"\n",
    "    blurb = f\"\\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    instruction = f\"### Instruct: {sample['instruction']}\"\n",
    "    response = f\"### Output: {sample['output']}\"\n",
    "    end = f\"### End\"\n",
    "\n",
    "    parts = [part for part in [blurb, instruction, response, end] if part]\n",
    "\n",
    "    formatted_prompt = \"\\n\\n\".join(parts)\n",
    "    sample[\"text\"] = formatted_prompt\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da670716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53746973",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "seed = 42 # TODO check this\n",
    "set_seed(seed)\n",
    "\n",
    "## Pre-process dataset\n",
    "max_length = get_max_length(original_model)\n",
    "print(max_length)\n",
    "\n",
    "train_dataset = preprocess_dataset(tokenizer, max_length,seed, train_dataset)\n",
    "eval_dataset = preprocess_dataset(tokenizer, max_length,seed, validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdd934f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32, \n",
    "    lora_alpha=32, \n",
    "    target_modules=[\n",
    "        'q_proj',\n",
    "        'k_proj',\n",
    "        'v_proj',\n",
    "        'dense'\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05, \n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\n",
    "original_model.gradient_checkpointing_enable()\n",
    "\n",
    "peft_model = get_peft_model(original_model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3522ef0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Using the prepare_model_for_kbit_training method from PEFT\n",
    "# Preparing the Model for QLoRA\n",
    "original_model = prepare_model_for_kbit_training(original_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745eac02",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f'./peft-protein-model-{str(int(time.time()))}'\n",
    "import transformers\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir = output_dir,\n",
    "    warmup_steps=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    max_steps=1000,\n",
    "    learning_rate=2e-4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    logging_steps=25,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=25,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=25,\n",
    "    do_eval=True,\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"none\",\n",
    "    overwrite_output_dir = True,\n",
    "    group_by_length=True,\n",
    "    disable_tqdm=False\n",
    ")\n",
    "\n",
    "peft_model.config.use_cache = False\n",
    "peft_model.requires_grad = True\n",
    "\n",
    "peft_trainer = transformers.Trainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    args=peft_training_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43ea3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1148ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                                      device_map='auto',\n",
    "                                                      quantization_config=bnb_config,\n",
    "                                                      trust_remote_code=True,\n",
    "                                                      token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cb4d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_tokenizer = AutoTokenizer.from_pretrained(model_name, add_bos_token=True, trust_remote_code=True, use_fast=False)\n",
    "eval_tokenizer.pad_token = eval_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a84ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "ft_model = PeftModel.from_pretrained(base_model, \"/mnt/TCC/protein-chat-flask/fine_tuning/peft-dialogue-summary-training-1728237910/checkpoint-1000\",torch_dtype=torch.float16,is_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c03307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(model,p, maxlen=100, sample=True):\n",
    "    toks = tokenizer(p, return_tensors=\"pt\")\n",
    "    res = model.generate(**toks.to(\"cuda\"), max_new_tokens=maxlen, do_sample=sample,num_return_sequences=1,temperature=0.01,num_beams=1,top_p=0.95,)\n",
    "    return tokenizer.batch_decode(res,skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55463af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from transformers import set_seed\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "index = 6\n",
    "instruction = test_dataset[index]['instruction']\n",
    "\n",
    "prompt = f\"Instruct: {instruction}\\nOutput:\\n\"\n",
    "\n",
    "peft_model_res = gen(ft_model,prompt,100,)\n",
    "peft_model_output = peft_model_res[0].split('Output:\\n')[1]\n",
    "#print(peft_model_output)\n",
    "prefix, success, result = peft_model_output.partition('###')\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(dash_line)\n",
    "print(f'PEFT MODEL:\\n{prefix}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadd87b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
