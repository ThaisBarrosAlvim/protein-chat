{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from pyarrow.dataset import dataset\n",
    "from sympy.codegen.ast import continue_\n",
    "!pip install -q -U bitsandbytes transformers peft accelerate datasets scipy einops evaluate trl rouge_score\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T00:06:17.014082Z",
     "start_time": "2024-10-07T00:06:11.506968Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    GenerationConfig, TFOpenAIGPTDoubleHeadsModel\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from trl import SFTTrainer\n",
    "import json\n",
    "import torch\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from huggingface_hub import interpreter_login\n",
    "from datasets import load_dataset\n",
    "\n",
    "interpreter_login()"
   ],
   "id": "af0d74cb06595c2c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
      "    Setting a new token will erase the existing one.\n",
      "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved in your configured git credential helpers (store).\n",
      "Your token has been saved to /home/thais/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T00:07:03.722867Z",
     "start_time": "2024-10-07T00:07:03.720664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "# disable Weights and Biases # TODO evaluate using WB\n",
    "os.environ['WANDB_DISABLED']=\"true\"\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF']=\"expandable_segments:True\""
   ],
   "id": "8d394435569e6f72",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T00:06:29.983923Z",
     "start_time": "2024-10-07T00:06:29.425786Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Carrega o dataset e divide ele em treino, validação e teste (TODO montar o dataset já desta forma)\n",
    "dataset = load_dataset('json', data_files='../data_construction/dataset_protein_articles.jsonl', split='train')\n",
    "\n",
    "# Definir proporções simplificadas (4:1:1) \n",
    "train_ratio = 4 / (4 + 1 + 1)\n",
    "test_ratio = 1 / (4 + 1 + 1)\n",
    "validation_ratio = 1 / (4 + 1 + 1)\n",
    "\n",
    "# Primeiro, dividir o dataset entre treino e (teste + validação)\n",
    "train_val_test = dataset.train_test_split(test_size=(1 - train_ratio))\n",
    "\n",
    "# Dividir a parte restante entre teste e validação\n",
    "test_val = train_val_test['test'].train_test_split(test_size=validation_ratio / (test_ratio + validation_ratio))\n",
    "\n",
    "# Obter os datasets de treino, teste e validação\n",
    "train_dataset = train_val_test['train']\n",
    "test_dataset = test_val['train']\n",
    "validation_dataset = test_val['test']\n",
    "\n",
    "# Exibir o tamanho de cada split\n",
    "print(f\"Tamanho do treino: {len(train_dataset)}\")\n",
    "print(f\"Tamanho do teste: {len(test_dataset)}\")\n",
    "print(f\"Tamanho da validação: {len(validation_dataset)}\")\n"
   ],
   "id": "74c5fbec5e902a0a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do treino: 20345\n",
      "Tamanho do teste: 5087\n",
      "Tamanho da validação: 5087\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T00:07:25.330795Z",
     "start_time": "2024-10-07T00:07:25.325824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "compute_dtype = getattr(torch, \"float16\")\n",
    "model_name='meta-llama/Llama-3.2-3B'\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")"
   ],
   "id": "dd077da0192aa057",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "device_map = {\"\": 0}\n",
    "original_model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                                      device_map=device_map,\n",
    "                                                      quantization_config=bnb_config,\n",
    "                                                      trust_remote_code=True,\n",
    "                                                      token=True)"
   ],
   "id": "fecf1527d00e2e3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T00:11:27.268003Z",
     "start_time": "2024-10-07T00:11:26.834023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name,trust_remote_code=True,padding_side=\"left\",add_eos_token=True,add_bos_token=True,use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ],
   "id": "2b5bab9db08d5f2",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from functools import partial\n",
    "\n",
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def get_max_length(model):\n",
    "    conf = model.config\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max lenth: {max_length}\")\n",
    "            break\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length\n",
    "\n",
    "\n",
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizing a batch\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int,seed, dataset):\n",
    "    \"\"\"Format & tokenize it so it is ready for training\n",
    "    :param tokenizer (AutoTokenizer): Model Tokenizer\n",
    "    :param max_length (int): Maximum number of tokens to emit from tokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    # Add prompt to each sample\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    dataset = dataset.map(create_prompt_formats)#, batched=True)\n",
    "\n",
    "    # Apply preprocessing to each batch of the dataset & and remove 'instruction', 'context', 'response', 'category' fields\n",
    "    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        remove_columns=['instruction', 'input', 'output'],\n",
    "    )\n",
    "\n",
    "    # Filter out samples that have input_ids exceeding max_length\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "\n",
    "    # Shuffle dataset\n",
    "    dataset = dataset.shuffle(seed=seed)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def create_prompt_formats(sample):\n",
    "    \"\"\"\n",
    "    Format various fields of the sample ('instruction','output')\n",
    "    Then concatenate them using two newline characters \n",
    "    :param sample: Sample dictionnary\n",
    "    \"\"\"\n",
    "    blurb = f\"\\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    instruction = f\"### Instruct: {sample['instruction']}\"\n",
    "    response = f\"### Output: {sample['output']}\"\n",
    "    end = f\"### End\"\n",
    "\n",
    "    parts = [part for part in [blurb, instruction, response, end] if part]\n",
    "\n",
    "    formatted_prompt = \"\\n\\n\".join(parts)\n",
    "    sample[\"text\"] = formatted_prompt\n",
    "\n",
    "    return sample"
   ],
   "id": "a928ea73c7963cae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "326813421a43cc33",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import set_seed\n",
    "seed = 42 # TODO check this\n",
    "set_seed(seed)\n",
    "\n",
    "## Pre-process dataset\n",
    "max_length = get_max_length(original_model)\n",
    "print(max_length)\n",
    "\n",
    "train_dataset = preprocess_dataset(tokenizer, max_length,seed, train_dataset)\n",
    "eval_dataset = preprocess_dataset(tokenizer, max_length,seed, validation_dataset)"
   ],
   "id": "c3f357dd5a597e7b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32, \n",
    "    lora_alpha=32, \n",
    "    target_modules=[\n",
    "        'q_proj',\n",
    "        'k_proj',\n",
    "        'v_proj',\n",
    "        'dense'\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05, \n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\n",
    "original_model.gradient_checkpointing_enable()\n",
    "\n",
    "peft_model = get_peft_model(original_model, config)"
   ],
   "id": "5af2aa7f8e6a21f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2 - Using the prepare_model_for_kbit_training method from PEFT\n",
    "# Preparing the Model for QLoRA\n",
    "original_model = prepare_model_for_kbit_training(original_model)"
   ],
   "id": "e2a05b94c2db0f92",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "output_dir = f'./peft-dialogue-summary-training-{str(int(time.time()))}'\n",
    "import transformers\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir = output_dir,\n",
    "    warmup_steps=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    max_steps=1000,\n",
    "    learning_rate=2e-4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    logging_steps=25,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=25,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=25,\n",
    "    do_eval=True,\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"none\",\n",
    "    overwrite_output_dir = True,\n",
    "    group_by_length=True,\n",
    ")\n",
    "\n",
    "peft_model.config.use_cache = False\n",
    "peft_model.requires_grad = True\n",
    "\n",
    "peft_trainer = transformers.Trainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    args=peft_training_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")"
   ],
   "id": "4095daa6c7d5c5e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "peft_trainer.train()",
   "id": "70e20724841c581",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T00:07:44.738973Z",
     "start_time": "2024-10-07T00:07:29.955399Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                                      device_map='auto',\n",
    "                                                      quantization_config=bnb_config,\n",
    "                                                      trust_remote_code=True,\n",
    "                                                      token=True)"
   ],
   "id": "a3d8d14c79f9cdc2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.90s/it]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T00:07:50.575805Z",
     "start_time": "2024-10-07T00:07:50.057157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "eval_tokenizer = AutoTokenizer.from_pretrained(model_name, add_bos_token=True, trust_remote_code=True, use_fast=False)\n",
    "eval_tokenizer.pad_token = eval_tokenizer.eos_token"
   ],
   "id": "4c96dbe7a25f910a",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T00:07:52.379771Z",
     "start_time": "2024-10-07T00:07:51.927949Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "ft_model = PeftModel.from_pretrained(base_model, \"/mnt/TCC/protein-chat-flask/fine_tuning/peft-dialogue-summary-training-1728237910/checkpoint-1000\",torch_dtype=torch.float16,is_trainable=False)"
   ],
   "id": "5e4647682a7960f",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T00:15:24.491650Z",
     "start_time": "2024-10-07T00:15:24.488691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def gen(model,p, maxlen=100, sample=True):\n",
    "    toks = tokenizer(p, return_tensors=\"pt\")\n",
    "    res = model.generate(**toks.to(\"cuda\"), max_new_tokens=maxlen, do_sample=sample,num_return_sequences=1,temperature=0.01,num_beams=1,top_p=0.95,)\n",
    "    return tokenizer.batch_decode(res,skip_special_tokens=True)"
   ],
   "id": "c527cd021513634e",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T00:16:43.686093Z",
     "start_time": "2024-10-07T00:16:39.252804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%time\n",
    "from transformers import set_seed\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "index = 6\n",
    "instruction = test_dataset[index]['instruction']\n",
    "\n",
    "prompt = f\"Instruct: {instruction}\\nOutput:\\n\"\n",
    "\n",
    "peft_model_res = gen(ft_model,prompt,100,)\n",
    "peft_model_output = peft_model_res[0].split('Output:\\n')[1]\n",
    "#print(peft_model_output)\n",
    "prefix, success, result = peft_model_output.partition('###')\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(dash_line)\n",
    "print(f'PEFT MODEL:\\n{prefix}')"
   ],
   "id": "988baab0b16c060",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "Instruct: What is the treatment of various human malignancies including testicular, ovarian, cervical, and non-small cell lung cancers?\n",
      "Output:\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "PEFT MODEL:\n",
      "1. Testicular cancer: Chemotherapy with cisplatin and vinblastine is the standard treatment. For patients with metastatic disease, chemotherapy with cisplatin and vinblastine is the standard treatment. For patients with metastatic disease, chemotherapy with cisplatin and vinblastine is the standard treatment. For patients with metastatic disease, chemotherapy with cisplatin and vinblastine is the standard treatment. For patients with metastatic disease, chemotherapy with cisplatin and vinblastine\n",
      "CPU times: user 4.42 s, sys: 12.4 ms, total: 4.43 s\n",
      "Wall time: 4.43 s\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f79bf2e37029bcd4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
